{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отток клиентов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализ оттока банковских клиентов\n",
    "Постройте модель с предельно большим значением *F1*-меры > 0.59.\n",
    "\n",
    "Источник данных: [https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling](https://www.kaggle.com/barelydedicated/bank-customer-churn-modeling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Признаки\n",
    "\n",
    "    RowNumber — индекс строки в данных\n",
    "    CustomerId — id клиента\n",
    "    Surname — фамилия\n",
    "    CreditScore — кредитный рейтинг\n",
    "    Geography — страна\n",
    "    Gender — пол\n",
    "    Age — возраст\n",
    "    Tenure — недвижимость клиента\n",
    "    Balance — баланс на счёте\n",
    "    NumOfProducts — количество продуктов банка, используемых клиентом\n",
    "    HasCrCard — наличие кредитной карты\n",
    "    IsActiveMember — активность клиента\n",
    "    EstimatedSalary — предполагаемая зарплата\n",
    "    \n",
    "#### Целевой признак\n",
    "\n",
    "    Exited — факт ухода клиента"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Загружаем датасет\n",
    "data = pd.read_csv('Churn.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           9091 non-null   float64\n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_view(df):\n",
    "    \"\"\"\n",
    "    Функция проходится по столбцам df и считает количество пропущенных, нулевых и отрицательных значений\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        na = df[column].isna().sum()\n",
    "        #В столбцах могут быть текстовые значения, поэтому используем try except\n",
    "        try: \n",
    "            zero = df[df[column] == 0][column].count()\n",
    "            negative = df[df[column] < 0][column].count()\n",
    "        except TypeError:\n",
    "            zero = 0\n",
    "            negative = 0\n",
    "            \n",
    "        \n",
    "        if na>0 or zero>0 or negative>0:\n",
    "            print()\n",
    "            print('********В столбце', column, 'обнаружено: ********')\n",
    "        if na > 0:\n",
    "            print(na, 'пропущеных значений')\n",
    "        if zero > 0:\n",
    "            print(zero, 'нулевых значений.')\n",
    "        if negative >0:\n",
    "            print(negative, 'отрицательных значений')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********В столбце Tenure обнаружено: ********\n",
      "909 пропущеных значений\n",
      "382 нулевых значений.\n",
      "\n",
      "********В столбце Balance обнаружено: ********\n",
      "3617 нулевых значений.\n",
      "\n",
      "********В столбце HasCrCard обнаружено: ********\n",
      "2945 нулевых значений.\n",
      "\n",
      "********В столбце IsActiveMember обнаружено: ********\n",
      "4849 нулевых значений.\n",
      "\n",
      "********В столбце Exited обнаружено: ********\n",
      "7963 нулевых значений.\n"
     ]
    }
   ],
   "source": [
    "first_view(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Датасете имеются пропущенные значения в столбце Tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Приведем названия столбцов к нижнему регистру\n",
    "data = data.set_axis(data.columns.str.lower(), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rownumber</th>\n",
       "      <th>customerid</th>\n",
       "      <th>surname</th>\n",
       "      <th>creditscore</th>\n",
       "      <th>geography</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>numofproducts</th>\n",
       "      <th>hascrcard</th>\n",
       "      <th>isactivemember</th>\n",
       "      <th>estimatedsalary</th>\n",
       "      <th>exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rownumber  customerid   surname  creditscore geography  gender  age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "\n",
       "   tenure    balance  numofproducts  hascrcard  isactivemember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "\n",
       "   estimatedsalary  exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для заполнения пропусков, так как, по моему, нету связи между значениями столбца Tenure и значениями в остальных столбцах, можно воспользоваться средней величиной или медианой, но тогда частоты сместятся к средним или медианным значениям. Я заполню пропуски случайными величинами, зависящими от частоты того или иного значения столбца в непропущенных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0     952\n",
       "2.0     950\n",
       "8.0     933\n",
       "3.0     928\n",
       "5.0     927\n",
       "7.0     925\n",
       "4.0     885\n",
       "9.0     882\n",
       "6.0     881\n",
       "10.0    446\n",
       "0.0     382\n",
       "Name: tenure, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Определим список уникальных значений столбца Tenure\n",
    "data['tenure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создание списков значений Tenure и вероятности того или иного значения\n",
    "prob_tenure = []\n",
    "tenure_list = []\n",
    "for i in range(0,11):\n",
    "    tenure_list.append(i)\n",
    "    prob_tenure.append((data['tenure'].value_counts()[i] / (len(data) - data['tenure'].isna().sum())).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.042, 0.105, 0.104, 0.102, 0.097, 0.102, 0.097, 0.102, 0.103, 0.097, 0.049] [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "print(prob_tenure, tenure_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       9\n",
       "1       3\n",
       "2       2\n",
       "3       2\n",
       "4       6\n",
       "       ..\n",
       "9995    7\n",
       "9996    6\n",
       "9997    0\n",
       "9998    3\n",
       "9999    8\n",
       "Length: 10000, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Создадим массив случайных чисел и объект Series который, в дальнейшем, присоединим к датафрейму\n",
    "random_array = np.random.RandomState(12345).choice(tenure_list, size=10000, p=prob_tenure)\n",
    "random_tenure = pd.Series(random_array, data.index)\n",
    "random_tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rownumber</th>\n",
       "      <th>customerid</th>\n",
       "      <th>surname</th>\n",
       "      <th>creditscore</th>\n",
       "      <th>geography</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>numofproducts</th>\n",
       "      <th>hascrcard</th>\n",
       "      <th>isactivemember</th>\n",
       "      <th>estimatedsalary</th>\n",
       "      <th>exited</th>\n",
       "      <th>tenure_rand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rownumber  customerid   surname  creditscore geography  gender  age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "\n",
       "   tenure    balance  numofproducts  hascrcard  isactivemember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "\n",
       "   estimatedsalary  exited  tenure_rand  \n",
       "0        101348.88       1            9  \n",
       "1        112542.58       0            3  \n",
       "2        113931.57       1            2  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tenure_rand'] = random_tenure\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   rownumber        10000 non-null  int64  \n",
      " 1   customerid       10000 non-null  int64  \n",
      " 2   surname          10000 non-null  object \n",
      " 3   creditscore      10000 non-null  int64  \n",
      " 4   geography        10000 non-null  object \n",
      " 5   gender           10000 non-null  object \n",
      " 6   age              10000 non-null  int64  \n",
      " 7   tenure           10000 non-null  float64\n",
      " 8   balance          10000 non-null  float64\n",
      " 9   numofproducts    10000 non-null  int64  \n",
      " 10  hascrcard        10000 non-null  int64  \n",
      " 11  isactivemember   10000 non-null  int64  \n",
      " 12  estimatedsalary  10000 non-null  float64\n",
      " 13  exited           10000 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "#Заполним пропуски в столбце Tenure и удалим вспомогательный столбец\n",
    "data['tenure'].fillna(data['tenure_rand'], inplace=True)\n",
    "del data['tenure_rand']\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Приведем данные к лучшим типам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   rownumber        10000 non-null  int64 \n",
      " 1   customerid       10000 non-null  int64 \n",
      " 2   surname          10000 non-null  object\n",
      " 3   creditscore      10000 non-null  int16 \n",
      " 4   geography        10000 non-null  object\n",
      " 5   gender           10000 non-null  object\n",
      " 6   age              10000 non-null  int8  \n",
      " 7   tenure           10000 non-null  int8  \n",
      " 8   balance          10000 non-null  int64 \n",
      " 9   numofproducts    10000 non-null  int8  \n",
      " 10  hascrcard        10000 non-null  int8  \n",
      " 11  isactivemember   10000 non-null  int8  \n",
      " 12  estimatedsalary  10000 non-null  int64 \n",
      " 13  exited           10000 non-null  int8  \n",
      "dtypes: int16(1), int64(4), int8(6), object(3)\n",
      "memory usage: 625.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data['tenure'] = data['tenure'].astype('int8')\n",
    "data['creditscore'] = data['creditscore'].astype('int16')\n",
    "data['numofproducts'] = data['numofproducts'].astype('int8')\n",
    "data['hascrcard'] = data['hascrcard'].astype('int8')\n",
    "data['isactivemember'] = data['isactivemember'].astype('int8')\n",
    "data['exited'] = data['exited'].astype('int8')\n",
    "data['age'] = data['age'].astype('int8')\n",
    "data['estimatedsalary'] = data['estimatedsalary'].astype('int64')\n",
    "data['balance'] = data['balance'].astype('int64')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовим данные для моделирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сначала избавимся от признаков которые точно не влияют на уход клиентов, это:\n",
    "1. RowNumber\n",
    "2. CustomerID\n",
    "3. Surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre_ohe = data.drop(['rownumber', 'customerid', 'surname'] , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Преобразуем категориальные признаки в численные методом OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ohe = pd.get_dummies(data_pre_ohe, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сформируем тренировочную валидационную и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделим признаки и целевой признак\n",
    "target = data_ohe['exited']\n",
    "features = data_ohe.drop(['exited'] , axis=1)\n",
    "    \n",
    "#Сформируем тестовую выборку\n",
    "features_train_valid, features_test, target_train_valid, target_test = train_test_split(\n",
    "    features, \n",
    "    target, \n",
    "    test_size=0.2, \n",
    "    random_state=12345)\n",
    "    \n",
    "#Сформируем обучающую и валидационную выборки\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(\n",
    "    features_train_valid, \n",
    "    target_train_valid, \n",
    "    test_size=0.25, \n",
    "    random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проведем масштабирование числовых признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выделим числовые признаки\n",
    "numeric = ['creditscore', 'age', 'tenure', 'balance', 'numofproducts', 'estimatedsalary']\n",
    "    \n",
    "#Обучим объект структуры StandartScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train[numeric])\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_valid[numeric] = scaler.transform(features_valid[numeric])\n",
    "features_test[numeric] = scaler.transform(features_test[numeric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод по шагу 1.\n",
    "1. Зполнены пропуски в столбце Tenure.\n",
    "2. Названия столбцов приведены к нижнему регистру.\n",
    "3. Все данные приведены к нужным типам.\n",
    "4. Из анализа исключены столбцы не влияющие на целевой признак.\n",
    "5. Категориальные признаки приведены к численным методом OHE.\n",
    "6. Выделен целевой признак - exited - факт ухода клиента.\n",
    "7. Данные разбиты на тренировочную, валидационную и тестовую выборки.\n",
    "8. Проведено масштабирование числовых признаков.\n",
    "\n",
    "Данные готовы к созданию моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Исследование задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Посмотрим на баланс классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7963\n",
       "1    2037\n",
       "Name: exited, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['exited'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Данных о уходе клиентов меньше почти в 4 раза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Построим модели обучения и посмотрим на их качество без балансировки классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(features_train, target_train, features_valid, target_valid, weight = None):\n",
    "    \"\"\"\n",
    "    Функция формирует модели для обучения, подбирает необходимые параметры моделей.\n",
    "    На выходе функции получим значения F1_score и AUC-ROC для Логистической регрессии\n",
    "    Дерева решений и Случайного леса.\n",
    "    Для дальнейшего выполнения задачи предусмотрено изменения параметра class_weight\n",
    "    \"\"\"\n",
    "       \n",
    "    #Обучим модель логистической регрессии\n",
    "    model_logistic = LogisticRegression(\n",
    "        random_state = 12345, \n",
    "        solver='liblinear', \n",
    "        class_weight = weight)\n",
    "    \n",
    "    model_logistic.fit(\n",
    "        features_train, \n",
    "        target_train)\n",
    "    \n",
    "    #Получим предсказания для валидационной выборки\n",
    "    prediction_logistic = model_logistic.predict(features_valid)\n",
    "    \n",
    "    #Оценим предсказания расчетом F1-score и AUC-ROC\n",
    "    \n",
    "    f1_logistic = f1_score(\n",
    "        target_valid, \n",
    "        prediction_logistic)\n",
    "    \n",
    "    probabilities_valid = model_logistic.predict_proba(features_valid)\n",
    "    \n",
    "    probabilities_one_valid = probabilities_valid[:, 1]\n",
    "    \n",
    "    auc_roc_logistic = roc_auc_score(\n",
    "        target_valid, \n",
    "        probabilities_one_valid)\n",
    "\n",
    "    print('Результаты предсказания Логистической регрессии')\n",
    "    print('F1_score =', f1_logistic)\n",
    "    print('AUC-ROC =', auc_roc_logistic)\n",
    "\n",
    "    #Обучим модель дерева решений, для этого сначала выбираем наилучшее значение max_depth\n",
    "    depth = tree_depth(\n",
    "        features_train, \n",
    "        features_valid, \n",
    "        target_train, \n",
    "        target_valid, \n",
    "        weight)\n",
    "    \n",
    "    model_tree = DecisionTreeClassifier(\n",
    "        max_depth=depth, \n",
    "        random_state=12345, \n",
    "        class_weight = weight)\n",
    "    \n",
    "    model_tree.fit(features_train, target_train)\n",
    "    prediction_tree = model_tree.predict(features_valid)\n",
    "    \n",
    "    #Оценим предсказания расчетом F1-score и AUC-ROC\n",
    "    \n",
    "    f1_tree = f1_score(target_valid, prediction_tree)\n",
    "    \n",
    "    probabilities_valid = model_tree.predict_proba(features_valid)\n",
    "    probabilities_one_valid = probabilities_valid[:, 1]\n",
    "    \n",
    "    auc_roc_tree = roc_auc_score(\n",
    "        target_valid, \n",
    "        probabilities_one_valid)\n",
    "    \n",
    "    print('Результаты предсказания Дерева решений')\n",
    "    print('F1_score =', f1_tree)\n",
    "    print('AUC-ROC =', auc_roc_tree)\n",
    "    \n",
    "    #Обучим модель случайного леса, для этого выберем наилучшие параметры max_depth и n_estimators\n",
    "    depth, estim = forest_depth_estim(\n",
    "        features_train, \n",
    "        features_valid, \n",
    "        target_train, \n",
    "        target_valid, \n",
    "        weight)\n",
    "    \n",
    "    model_forest = RandomForestClassifier(\n",
    "        max_depth = depth, \n",
    "        n_estimators=estim, \n",
    "        random_state=12345, \n",
    "        class_weight = weight)\n",
    "    \n",
    "    model_forest.fit(features_train, target_train)\n",
    "    prediction_forest = model_forest.predict(features_valid)\n",
    "    \n",
    "    #Оценим предсказания расчетом F1-score и AUC-ROC\n",
    "    \n",
    "    f1_forest = f1_score(target_valid, prediction_forest)\n",
    "    \n",
    "    probabilities_valid = model_forest.predict_proba(features_valid)\n",
    "    probabilities_one_valid = probabilities_valid[:, 1]\n",
    "    auc_roc_forest = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "    \n",
    "    print('Результаты предсказания Случайного леса')\n",
    "    print('F1_score =', f1_forest)\n",
    "    print('AUC-ROC =', auc_roc_forest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_depth(features_train, features_valid, target_train, target_valid, weight = None):\n",
    "    \"\"\"\n",
    "    Функция на вход получает вылидационную и обучающие выборки а на выходе получает наилучшее значение\n",
    "    max_depth, с которым F1-score максимально.\n",
    "    \"\"\"\n",
    "    best_depth = 0\n",
    "    max_f1 = 0\n",
    "    \n",
    "    for d in range(1,15):\n",
    "        model = DecisionTreeClassifier(\n",
    "            max_depth=d, \n",
    "            random_state=12345, \n",
    "            class_weight = weight)\n",
    "        \n",
    "        model.fit(features_train, target_train)\n",
    "        predicted_valid = model.predict(features_valid)\n",
    "        if max_f1 < f1_score(target_valid, predicted_valid):\n",
    "            max_f1 = f1_score(target_valid, predicted_valid)\n",
    "            best_depth = d\n",
    "    print()\n",
    "    print('Наилучшее значение max_depth =', best_depth, ', с ним f1_score =', max_f1)\n",
    "    print()\n",
    "    return best_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forest_depth_estim(features_train, features_valid, target_train, target_valid, weight = None):\n",
    "    \"\"\"\n",
    "    Функция на вход получает вылидационную и обучающие выборки а на выходе получает наилучшее значение\n",
    "    max_depth и n_estimators по максимальному значению F1-score.\n",
    "    \"\"\"\n",
    "      \n",
    "    best_depth = 0\n",
    "    max_f1 = 0\n",
    "    best_estim = 0\n",
    "    for d in range(1,15):\n",
    "        for e in range(10, 100, 10):\n",
    "            \n",
    "            model = RandomForestClassifier(\n",
    "                max_depth=d, \n",
    "                n_estimators=e, \n",
    "                random_state=12345, \n",
    "                class_weight = weight)\n",
    "            \n",
    "            model.fit(features_train, target_train)\n",
    "            predicted_valid = model.predict(features_valid)\n",
    "            if max_f1 < f1_score(target_valid, predicted_valid):\n",
    "                max_f1 = f1_score(target_valid, predicted_valid)\n",
    "                best_depth = d\n",
    "                best_estim = e\n",
    "    print()\n",
    "    print('Наилучшее значение max_depth =', best_depth, 'n_estimators =', best_estim, ', с ними f1_score =', max_f1)\n",
    "    print()\n",
    "    return best_depth, best_estim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты предсказания Логистической регрессии\n",
      "F1_score = 0.3007518796992481\n",
      "AUC-ROC = 0.7703184930037084\n",
      "\n",
      "Наилучшее значение max_depth = 7 , с ним f1_score = 0.5557299843014128\n",
      "\n",
      "Результаты предсказания Дерева решений\n",
      "F1_score = 0.5557299843014128\n",
      "AUC-ROC = 0.8244529254401789\n",
      "\n",
      "Наилучшее значение max_depth = 13 n_estimators = 30 , с ними f1_score = 0.5700636942675159\n",
      "\n",
      "Результаты предсказания Случайного леса\n",
      "F1_score = 0.5700636942675159\n",
      "AUC-ROC = 0.8361216240488685\n"
     ]
    }
   ],
   "source": [
    "prediction(features_train, target_train, features_valid, target_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод по шагу 2.\n",
    "1. Данных по ушедшим клиентам банка в 4 раза меньше чем по оставшимся.\n",
    "2. Составлены модели логистической регрессии, дерева решений и случайного леса, подобраны параметры модели.\n",
    "3. Для каждой модели подсчитаны F1_score и AUC_ROC. \n",
    "4. Ни одна из моделей не смогла дать предсказания с требуемым качеством f1_score = 0,59.\n",
    "5. Наилучшей моделью оказалась модель случайного леса f1_score = 0,57.\n",
    "6. Площадь под кривой ошибок для всех моделей больше 0,5, что говорит о адекватности моделей предсказаний, а также о том что предсказания всех трех моделей лучше предсказания случайной модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Борьба с дисбалансом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Для борьбы с дисбалансом воспользуемся тремя методами:\n",
    "    1. Сбалансируем вес класса\n",
    "    2. Увеличим выборку положительных объектов\n",
    "    3. Уменьшим выборку отрицательных объектов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты предсказания Логистической регрессии\n",
      "F1_score = 0.47899910634495085\n",
      "AUC-ROC = 0.7724595823683595\n",
      "\n",
      "Наилучшее значение max_depth = 6 , с ним f1_score = 0.5587044534412956\n",
      "\n",
      "Результаты предсказания Дерева решений\n",
      "F1_score = 0.5587044534412956\n",
      "AUC-ROC = 0.8090671240258203\n",
      "\n",
      "Наилучшее значение max_depth = 8 n_estimators = 60 , с ними f1_score = 0.5990675990675991\n",
      "\n",
      "Результаты предсказания Случайного леса\n",
      "F1_score = 0.5990675990675991\n",
      "AUC-ROC = 0.8533282256615997\n"
     ]
    }
   ],
   "source": [
    "#Для балансировки веса встроенными в sklearn методами, добавим параметр class_weight='balanced'.\n",
    "prediction(features_train, \n",
    "           target_train, \n",
    "           features_valid, \n",
    "           target_valid, \n",
    "           weight = 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проведем увеличение тренировочной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(features, target, repeat):\n",
    "    \"\"\"\n",
    "    Функция увеличивает выборку объектов с положительным целевым признаком в repeat раз.\n",
    "    \"\"\"\n",
    "    #Разделим выборку на объекты с положительным и отрицательным целевым признаком\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "    # Сформируем новую тренировочную выборку\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    # Перемешаем объекты в выборке\n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345)\n",
    "    \n",
    "    return features_upsampled, target_upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Увеличим в 3 раза."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_upsampled, target_upsampled = upsample(features_train, target_train, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучим модели на новой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты предсказания Логистической регрессии\n",
      "F1_score = 0.4778012684989429\n",
      "AUC-ROC = 0.7721114765251089\n",
      "\n",
      "Наилучшее значение max_depth = 6 , с ним f1_score = 0.5602605863192183\n",
      "\n",
      "Результаты предсказания Дерева решений\n",
      "F1_score = 0.5602605863192183\n",
      "AUC-ROC = 0.8118217698082556\n",
      "\n",
      "Наилучшее значение max_depth = 12 n_estimators = 70 , с ними f1_score = 0.6091503267973858\n",
      "\n",
      "Результаты предсказания Случайного леса\n",
      "F1_score = 0.6091503267973858\n",
      "AUC-ROC = 0.8474883130218607\n"
     ]
    }
   ],
   "source": [
    "prediction(\n",
    "    features_upsampled, \n",
    "    target_upsampled, \n",
    "    features_valid, \n",
    "    target_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучим модели с балансировкой веса классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты предсказания Логистической регрессии\n",
      "F1_score = 0.48035714285714287\n",
      "AUC-ROC = 0.772453224270766\n",
      "\n",
      "Наилучшее значение max_depth = 6 , с ним f1_score = 0.5587044534412956\n",
      "\n",
      "Результаты предсказания Дерева решений\n",
      "F1_score = 0.5587044534412956\n",
      "AUC-ROC = 0.80906712402582\n",
      "\n",
      "Наилучшее значение max_depth = 13 n_estimators = 90 , с ними f1_score = 0.6045918367346939\n",
      "\n",
      "Результаты предсказания Случайного леса\n",
      "F1_score = 0.6045918367346939\n",
      "AUC-ROC = 0.8464535326384991\n"
     ]
    }
   ],
   "source": [
    "prediction(\n",
    "    features_upsampled, \n",
    "    target_upsampled, \n",
    "    features_valid, \n",
    "    target_valid, \n",
    "    weight = 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проведем уменьшение тренировочной выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(features, target, fraction):\n",
    "    \"\"\"\n",
    "    Функция уменьшает выборку объектов с отрицательным целевым признаком в fraction раз.\n",
    "    \"\"\"\n",
    "    #Разделим выборку на объекты с положительным и отрицательным целевым признаком\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "    # Сформируем новую тренировочную выборку\n",
    "    features_downsampled = pd.concat(\n",
    "        [features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n",
    "    target_downsampled = pd.concat(\n",
    "        [target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n",
    "    # Перемешаем объекты в выборке\n",
    "    features_downsampled, target_downsampled = shuffle(\n",
    "        features_downsampled, target_downsampled, random_state=12345)\n",
    "    \n",
    "    return features_downsampled, target_downsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Уменьшим в 2 раза"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_downsampled, target_downsampled = downsample(features_train, target_train, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучим модели на новой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты предсказания Логистической регрессии\n",
      "F1_score = 0.490272373540856\n",
      "AUC-ROC = 0.7718555630969657\n",
      "\n",
      "Наилучшее значение max_depth = 7 , с ним f1_score = 0.5751633986928104\n",
      "\n",
      "Результаты предсказания Дерева решений\n",
      "F1_score = 0.5751633986928104\n",
      "AUC-ROC = 0.8155484097603157\n",
      "\n",
      "Наилучшее значение max_depth = 9 n_estimators = 90 , с ними f1_score = 0.6025104602510459\n",
      "\n",
      "Результаты предсказания Случайного леса\n",
      "F1_score = 0.6025104602510459\n",
      "AUC-ROC = 0.854350289849774\n"
     ]
    }
   ],
   "source": [
    "prediction(features_downsampled, target_downsampled, features_valid, target_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обучим модели с балансировкой веса классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты предсказания Логистической регрессии\n",
      "F1_score = 0.47069271758436954\n",
      "AUC-ROC = 0.772292682306527\n",
      "\n",
      "Наилучшее значение max_depth = 6 , с ним f1_score = 0.5586272640610105\n",
      "\n",
      "Результаты предсказания Дерева решений\n",
      "F1_score = 0.5586272640610105\n",
      "AUC-ROC = 0.8129455635579278\n",
      "\n",
      "Наилучшее значение max_depth = 13 n_estimators = 70 , с ними f1_score = 0.5967940813810111\n",
      "\n",
      "Результаты предсказания Случайного леса\n",
      "F1_score = 0.5967940813810111\n",
      "AUC-ROC = 0.8431608328472038\n"
     ]
    }
   ],
   "source": [
    "prediction(features_downsampled, target_downsampled, features_valid, target_valid, weight = 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы по шагу 3.\n",
    "1. Уменьшение дисбаланса классов увеличило f1_score у всех моделей.\n",
    "2. Балансировка классов не помогла моделям логистической регрессии и дерева решений достичь целевого показателя f1_score = 0,59.\n",
    "3. Любой из методов балансировки помогает модели случайного леса превысить требуемое значение f1_score = 0,59, при  этом разница в значениях не значительна. Наилучшее занчение f1_score = 0,609 получено при увеличении тренировочной выборки и параметрах модели - max_depth = 12 и n_estimators = 70.\n",
    "4. Метрика AUC-ROC, при балансировке:\n",
    "    - для логистической регрессии, почти не меняется,\n",
    "    - для решающего дерева всегда меньше чем при несбалансированных классах,\n",
    "    - для случайного леса всегда больше чем при несбалансированных классах.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Используем модель с наилучшими параметрами и  увеличенную выборку для построения финальной модели и ее тестирования."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты предсказания валидационной выборки\n",
      "F1_score = 0.6091503267973858\n",
      "AUC-ROC = 0.8474883130218607\n",
      "\n",
      "Результаты предсказания тестовой выборки\n",
      "F1_score = 0.6347305389221557\n",
      "AUC-ROC = 0.8609929563729861\n"
     ]
    }
   ],
   "source": [
    "model_final= RandomForestClassifier(\n",
    "    max_depth = 12, \n",
    "    n_estimators=70, \n",
    "    random_state=12345)\n",
    "\n",
    "model_final.fit(features_upsampled, target_upsampled)\n",
    "prediction_valid = model_final.predict(features_valid)\n",
    "prediction_test = model_final.predict(features_test)    \n",
    "#Оценим предсказания расчетом F1-score и AUC-ROC на валидационной и тестовой выборках\n",
    "    \n",
    "f1_valid = f1_score(target_valid, prediction_valid)\n",
    "f1_test = f1_score(target_test, prediction_test)    \n",
    "\n",
    "probabilities_valid = model_final.predict_proba(features_valid)\n",
    "probabilities_one_valid = probabilities_valid[:, 1]\n",
    "auc_roc_valid = roc_auc_score(target_valid, probabilities_one_valid)\n",
    "\n",
    "probabilities_test = model_final.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "auc_roc_test = roc_auc_score(target_test, probabilities_one_test)\n",
    "\n",
    "print('Результаты предсказания валидационной выборки')\n",
    "print('F1_score =', f1_valid)\n",
    "print('AUC-ROC =', auc_roc_valid)\n",
    "print()\n",
    "print('Результаты предсказания тестовой выборки')\n",
    "print('F1_score =', f1_test)\n",
    "print('AUC-ROC =', auc_roc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общий вывод.\n",
    "1. Проведена подкготовка данных к построению моделей.\n",
    "2. Проведено исследование различных моделей на несбалансированной выборке. Наилучшей оказалась модель случайного леса с f1_score = 0,57, что меньше требуемого значения 0,59.\n",
    "3. Проведена балансировка классов тремя способами:\n",
    "    - балансировкой веса,\n",
    "    - увеличением выборки,\n",
    "    - уменьшением выборки.\n",
    "4. На каждом этапе балансировки проводилось исследование качества моделей. Модель случайного леса достигала требуемого значения f1_score = 0,59 или выше при любом способое балансировки.\n",
    "5. Модель с наилучшеми характеристиками была выбрана для финального тестирования на тествоой выборке. Тестирование показало:\n",
    "    - на валидационной выборке F1_score = 0.609, AUC-ROC = 0.847,\n",
    "    - на тестовой выборке F1_score = 0.634, AUC-ROC = 0.861.\n",
    "6. Значение AUC-ROC говорят о том что полученная модель адекватна и делает предсказания лучше чем случайная."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
